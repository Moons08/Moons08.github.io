---
title: 1년차 데이터 엔지니어의 2019년 소회
date: 2019-12-31
tags: blog
category: blog
---

내년에는 월별, 혹은 분기별로 결산, 소회를 해야겠다. 일년치를 한번에 쓰려니 쉽지 않다.

## 1분기

18년 8월 말에 입사를 했으니 1월은 5개월 차에 접어드는 달이었다. 어느정도 회사에 적응했고, TF로 참여했던 프로젝트도 일단락 되었다.

1분기에 가장 매달렸던 일은 Airflow로 **내 업무**를 자동화하는 것이었다. 정기적으로 데이터를 입수, 적재, 가공 (a.k.a ETL)하는 여러 프로세스들이 오라클 쿼리, 제플린 노트북 등에 산재되어 있었고, 이 많은 작업들을 일정에 맞춰서 일일이 수동으로 작업하다보면 다른 일은 할 수가 없었기 때문에 이걸 정리해야만 했었다. 입사하고 처음 인수인계 받았을 때야 프로세스를 이해하기도 벅차서 (전임자가 남겨둔 문서가 많은 도움이 되었다) 도저히 할 수 없었지만, 이때쯤에는 해볼만하다 싶었던 것 같다.

물론 적용에는 많은 고난과 역경이 있었다. 내부망 환경이라 인터넷이 안되는 것부터, 한글 인코딩 에러라던지, 오라클 서버, 하둡 연결, … , 복잡한 Airflow 사용법도 한 몫 했다.  환경설정은 정말 khp 과장님의 적극적인 서포트 덕에 해낼 수 있었다. 환경설정이 끝난 후에는 기존 코드들을 최대한 airflow에서 써먹을 수 있도록, 멱등성을 가지도록 수정해야했다. 담당한 대부분의 프로세스들이 airflow에서 제대로 돌아갈 때쯤에는 해방이다! 싶었지만 완전히 모르고 살아도 될 정도로 해방된 것은 아니고, 몇몇 이유로 10% 정도의 부분은 남겨두게 되었다. 원래 있던 코드라는 이유만으로 굳이 고치지 않은 부분들이 가끔 새로운 이슈를 가지고 오기도 했다.

데이터 엔지니어링을 더 깊게 공부해보고자  **스파크를 활용한 파이프라인 만들기**를 수강했다. 스파크, 제플린은 회사에서 항상 사용하는 부분이라 쉽게 따라갈 수 있었고, 추가로 AWS 환경의 다양한 도구, ELK와의 연동, 그리고 데이터 엔지니어링, 파이프라인의 기본 흐름에 대해 배울 수 있었다.

**딥러닝을 위한 리눅스와 GPU** 강의도 들었다. 환경설정은 언제나 고난이 찾아오는데, 이 분야는 물어봐도 시원하게 답변이 오지 않기 때문에 시간을 절약해보고자 선택했다. 설치, 환경설정은 **docker**로 하자는 것이 내 나름의 결론. 강의 내용에는 리눅스와 vim 관련도 다소 포함되어 있었는데, 오히려 이 부분에서 많은 것을 얻어갈 수 있었다. 명령어는 자주 쓰는 것만 쓰게 되는데, 덕분에 쓰는 가짓수가 조금 늘었다.

## 2분기

4월에는 처음으로 긴 휴가를 보냈고, 다녀와서는 **Object Detection** 강의를 들었다. 분류, 회귀, 군집보다 한단계 복잡한 문제인 디텍션을 배우며 텐서플로우도 더 깊게 다루어 볼 수 있었다. 역시 이미지는 딥러닝이 잘 먹히는 분야가 아닌가 싶다. opencv와 pretrained model을 이용해서 친구들과 찍었던 영상에 face regcognition을 해보는 것으로 나름의 토이프로젝트를 마무리 했다. 이미지, 영상 데이터는 예제 데이터 말고는 만져본 적이 없었는데, 막상 해보니 그리 어려워만할 일은 아니었다. *이미지 이어 붙이면 그게 영상이지, 뭐.* 나름 깊게 배웠다고 생각해서 정리해야지 하다가도 강사님이 자료를 잘 만들어주셔서 그냥 베끼는게 아닌가 싶어서인지, 자꾸 미루게 되었다. 20년에는 복습할 겸 정리 해야겠다. 개인적으로 가장 기억에 남았던 부분은 focal loss.

코드 리팩토링을 했다. 이슈가 있어서 한 것도 있고, 어쩌다보니 눈에 들어와서 한 부분도 있다. 오래된 코드가 대다수이다 보니 건드리고 싶지도 않고, 고쳐서 조금 더 효율이 좋아진다고 무슨 부귀영화가 올까하는 생각이 들기도 했다. 특히 자동화 해놓은 프로세스면 더 그랬다. 그래도 빨리 끝나면 좋으니까, 하는 마음으로 진행했다.

1년 간의 인턴 기간 종료를 앞두고, 정규직 전환 면접이 있었다. 당시 그룹사 대상 강의의 한 세션을 맡게 되어서 이것도 준비하느라 정신 없이 지나갔던 것 같다. 다행히 합격하여 전환될 수 있었고, 강의도 만족스럽지는 않았지만 어찌저찌하여 넘어갈 수 있었다. ~~다들 처음은 망한다더라~~ 그렇게 아쉬운 세션을 뒤로하고 바로 그 다음 주에 Devground 행사에 갔다. 일주일만에 다시 반성할 수 있었다. 인터넷을 통해 (혼자) 자주 뵙는 분들도 있고, 처음 뵙는 분들도 계셨는데, 대단한 사람이 참 많구나 싶다. 유투브에 공개가 된 것도 있고, 안된 것도 있는데, 구글 코리아 백정상 님의 세션 중 다음 부분이 기억에 남는다.

> ML프로젝트를 실패하지 않으려면 프로젝트를 통해 얻는 business impact가 충분히 커야하며, 최소 4인 팀플이 필요하다.  

## 3분기

좋은 가격에 중고 macbook pro를 얻었다. 제대 후에 계속 써온 gram을 이제 보내줄 때도 되었고, swiftUI 데모를 보고 혹 하기도 했고, 아이폰, 아이패드에 이어 생태계의 hub를 경험해보고자 *앱등완성* 냉큼 구했다. gram은 이제 우분투를 지우고 다시 윈도우로 돌아가게 되었다. 은퇴는 없다. 맥북도 생겼으니 swift를 살짝 공부해봤는데, java같기도, python 같기도 하고 그렇다. swiftUI 한번 써보겠다고 카탈리나 베타일때 올려서 했는데, ~~터미널 안열리는 오류 덕분에 초기화도 경험했다.~~ 막상 정식 배포되고 나서는 한번도 안했다. 반성.

3분기에는 **Elastic Stack을 활용한 Data Dashboard 만들기**를 수강했다. 이전에 들은 강의들에 조금씩 포함되어 있어 ELK을 처음 접하는 것은 아니었다. 스택 중 Kibana에 집중된 강의였고, 팀에서 사용하는 데이터 대쉬보드를 키바나로 해볼까해서 듣게 되었다. 결국 이런 저런 이유로 적용하지는 못했지만, 완전히 독립적인 데이터 작업을 하게 된다면 그때 다시 꺼내볼 수 있을 것 같다. 일단 넓게 넓게 뭐가 있는지 많이 알고, 그 다음에 깊게 파야지.

**Google Study Jam**의 머신러닝 심화반을 수료했다. 기념품도 받았다. 두 개의 코세라 강의와 한개의 퀵랩 퀘스트를 진행했는데, 거의 반 정도는 GCP 공부였다. ~~물론 클라우드 공부도 하면 좋지~~ 퀵랩의 경우는 한달 무료 이용권을 주는데, 필요조건인 퀘스트 말고도 *쿠버네티스 라던지* 재밌어보이는게 많아서 이것저것 진행해봤다. 다만 실습 환경이 **너무** 잘되있어서 시키는 대로 복사-붙여넣기만 하면 진행이 되다보니, 막상 다 끝나고나서 머리에 남는게 별로 없는 느낌이었다.

## 4분기

**JavaScript Boot Camp**를 수강했다. 갑자기 프론트엔드 개발자가 되고 싶었던 것은 아니고, 프로그래밍을 잘하고 싶어서 신청했다. 이제 프론트 쪽 얘기를 들으면 아, 그런게 있었지 정도는 할 것 같다. 강의 마지막 주에는 react 프레임워크도 살짝 다뤘는데, 꽤 재밌었다. 본격적으로하면 개발하는 재미가 있을 것 같다. 다만 강의 이후에 뭘 만들어내지는 못해서 마무리가 좀 아쉽다. 복습하면서 블로그에 정리도 하고, 간단한 토이 프로젝트 하나 해야겠다.

컴공 스터디를 시작했다. 결과적으로 정말 잘한 선택이었다. 프로그래밍을 하는데 필수는 아니라지만, 알면 좋다는 말에 얼마나 좋길래 라는 궁금증을 가지고 운영체제부터 공부를 시작했다. 어렴풋이 알고 있던 개념도 있었고, 아예 처음 뵙는 개념도 있었는데, 재밌게 강의를 들었다. 그리고 이런 것도 모르고 코드를 써왔구나 싶었다. (함께 스터디한 분도 동감했다)
OS를 훑고 나서는 컴퓨터 구조 강의를 봤는데, 조금 덜 실용적인 느낌이라 다음으로 미루고 자료구조, 알고리즘 과목을 공부하기로 했다. 마침 듣기로 한 강의가 C를 예제로 해서 C 언어도 공부할 수 있었다. 한번 듣고 leetcode 예제 푸는데 아 이거 생각보다 쉽지 않다. 문제풀면서 복습도 간간히 해야겠다. 최근에는 네트워크 강의를 듣고 있다. 

회사에서는 데이터 검수를 위해 Tensorflow data validation(TFDV)를 일부 처리 과정에 시범적으로 적용했다. 나머지 대상 데이터에 적용에는 데이터도 많고, 형식도 각양각색이다보니 작업에 다소 고난을 겪고 있는데, 일단은 스키마를 저장할 수 있는 parquet 형식의 데이터를 읽고, 샘플링해서, 통계치를 생성하는 방법으로 airflow subdag를 작성하고 있다. 자동화 해둔 프로세스 앞, 뒤에 붙여서 쉽게 갖다 쓸 수 있게 하는 것이 목표.

## 마무리

올해는 나름 이것저것 많이 해보려고 했던 것 같다. 많이 하는 것 보다는 잘 하는게 당연히 더 중요하긴 한데, 내가 잘하고 싶은 것을 찾는 과정이라고 생각하려고 한다. 올해 가장 잘한 부분은 역시 컴공 스터디를 시작한 것. 근본 없는 개발자에서 벗어나기 위한 노오력이라고 할 수 있겠다. 운영체제를 첫 시작으로 삼은 것도 잘한 일이었다. 아쉬운 부분은 공부한 것에 대해 마무리를 제대로 못한 부분이 많은 것. 그때 그때 잘 정리해두지 않으면 다 까먹는데, 욕심이 많아 벌려놓긴 했는데, 소화를 못한 부분이 좀 있다.

2020년에는 자격증 공부를 할 것이다. 하나는 AWS 어소시에이트 자격증, 다른 하나는 정보처리기사. AWS 자격증은 멋있기도 하고, 실용적이기도 해서 목표로 하고 있고*re:Invent에 간다던지*, 정보처리기사는 삶에 무슨 일이 생길지 모르니 취득할 예정이다. 두 번째 목표는 개발 관련 외의 글도 주기적으로 관리하면서 포스팅 하는 것. 따로 블로그를 생성할 지는 고민해봐야겠다. 세 번째 목표는 올해 공부한 것과 내년에 공부할 것들로 사이드 프로젝트를 하는 것. 끝.
